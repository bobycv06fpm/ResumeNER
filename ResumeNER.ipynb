{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ResumeNER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKu_88Sb9a5C"
      },
      "source": [
        "# !git clone https://github.com/nirajkale/ResumeNER.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4okkvXn9of2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ee8604-fe80-4fed-ff49-e9ea06fa7cf3"
      },
      "source": [
        "!pip install -r /content/ResumeNER/req.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval==0.0.12 in /usr/local/lib/python3.6/dist-packages (from -r /content/ResumeNER/req.txt (line 1)) (0.0.12)\n",
            "Requirement already satisfied: transformers==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/ResumeNER/req.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from -r /content/ResumeNER/req.txt (line 4)) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval==0.0.12->-r /content/ResumeNER/req.txt (line 1)) (1.19.4)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval==0.0.12->-r /content/ResumeNER/req.txt (line 1)) (2.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0->-r /content/ResumeNER/req.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0->-r /content/ResumeNER/req.txt (line 2)) (20.8)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0->-r /content/ResumeNER/req.txt (line 2)) (0.1.94)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0->-r /content/ResumeNER/req.txt (line 2)) (0.8.1rc2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0->-r /content/ResumeNER/req.txt (line 2)) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0->-r /content/ResumeNER/req.txt (line 2)) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0->-r /content/ResumeNER/req.txt (line 2)) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0->-r /content/ResumeNER/req.txt (line 2)) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0->-r /content/ResumeNER/req.txt (line 2)) (0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->-r /content/ResumeNER/req.txt (line 4)) (51.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r /content/ResumeNER/req.txt (line 4)) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->-r /content/ResumeNER/req.txt (line 4)) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r /content/ResumeNER/req.txt (line 4)) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r /content/ResumeNER/req.txt (line 4)) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r /content/ResumeNER/req.txt (line 4)) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->-r /content/ResumeNER/req.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r /content/ResumeNER/req.txt (line 4)) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r /content/ResumeNER/req.txt (line 4)) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r /content/ResumeNER/req.txt (line 4)) (3.0.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval==0.0.12->-r /content/ResumeNER/req.txt (line 1)) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval==0.0.12->-r /content/ResumeNER/req.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval==0.0.12->-r /content/ResumeNER/req.txt (line 1)) (3.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0->-r /content/ResumeNER/req.txt (line 2)) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0->-r /content/ResumeNER/req.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0->-r /content/ResumeNER/req.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0->-r /content/ResumeNER/req.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.1.0->-r /content/ResumeNER/req.txt (line 2)) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.1.0->-r /content/ResumeNER/req.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.1.0->-r /content/ResumeNER/req.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.1.0->-r /content/ResumeNER/req.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->-r /content/ResumeNER/req.txt (line 4)) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->-r /content/ResumeNER/req.txt (line 4)) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->-r /content/ResumeNER/req.txt (line 4)) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4wdKFrvgLGs"
      },
      "source": [
        "from tensorflow.python.keras.backend import dropout\n",
        "from ResumeNER import *\n",
        "import transformers\n",
        "from transformers import ElectraTokenizer, TFElectraModel\n",
        "from os import path\n",
        "import tensorflow as tf\n",
        "import math\n",
        "from tensorflow import optimizers\n",
        "import numpy as np\n",
        "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLBglPCygX-0"
      },
      "source": [
        "seed = 232\n",
        "model_name = 'google/electra-base-discriminator'\n",
        "tokenizer = ElectraTokenizer.from_pretrained(model_name)\n",
        "use_token_type_ids = \"token_type_ids\" in tokenizer.model_input_names\n",
        "\n",
        "use_iob2_format = True\n",
        "model_meta = ModelMeta()\n",
        "model_meta.model_type = 'bert'\n",
        "batch_size = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDQ-qMvwgbZp"
      },
      "source": [
        "def read_data(filepath):\n",
        "    examples, annotations_list, class_list = read_annotation_file(filepath)\n",
        "    converted_examples = convert_platform_data_to_ner(examples, annotations_list, class_list, use_iob2_format = use_iob2_format)\n",
        "    class_map = {i:label for i, label in enumerate(class_list)}\n",
        "    features = convert_examples_to_features(model_meta, converted_examples,class_list,tokenizer,use_iob2_format = use_iob2_format)\n",
        "    return features, class_map\n",
        "\n",
        "def create_tensorflow_dataset(features):\n",
        "    def gen():\n",
        "        for ex in features:\n",
        "            yield (\n",
        "                {\n",
        "                    \"input_ids\": ex.input_ids,\n",
        "                    \"attention_mask\": ex.attention_mask,\n",
        "                    \"token_type_ids\": ex.token_type_ids,\n",
        "                },\n",
        "                ex.label_ids,\n",
        "            )\n",
        "    return tf.data.Dataset.from_generator(\n",
        "            gen,\n",
        "            ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
        "            (\n",
        "                {\n",
        "                    \"input_ids\": tf.TensorShape([None]),\n",
        "                    \"attention_mask\": tf.TensorShape([None]),\n",
        "                    \"token_type_ids\": tf.TensorShape([None]),\n",
        "                },\n",
        "                tf.TensorShape([None]),\n",
        "            ),\n",
        "        )\n",
        "    \n",
        "def build_model(num_labels, use_dropout=True, dropout_rate=0.15):\n",
        "    model = TFElectraModel.from_pretrained(model_name)\n",
        "    input_ids = tf.keras.layers.Input(shape=(model_meta.max_seq_length,), name='input_ids', dtype='int32')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(model_meta.max_seq_length,), name='attention_mask', dtype='int32')\n",
        "    token_type_ids = tf.keras.layers.Input(shape=(model_meta.max_seq_length,), name='token_type_ids', dtype='int32')\n",
        "    model_inputs = [input_ids, attention_mask, token_type_ids]\n",
        "    outputs = model(model_inputs)\n",
        "    logits = outputs[0]\n",
        "    if use_dropout and dropout_rate>0:\n",
        "        logits = tf.keras.layers.Dropout(dropout_rate)(logits)\n",
        "    model_op = tf.keras.layers.Dense(num_labels, activation = 'softmax', kernel_initializer='glorot_uniform')(logits)\n",
        "    keras_model = tf.keras.Model(inputs= model_inputs, outputs = model_op)\n",
        "    return keras_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EReCIscvgmm6",
        "outputId": "e354e536-0da7-417c-aa82-78ab55906273"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "features, class_map = read_data([r'/content/ResumeNER/traindata.json', r'/content/ResumeNER/testdata.json'])\n",
        "print(class_map)\n",
        "features_train,features_test = train_test_split(features, test_size=0.2, shuffle=True)\n",
        "\n",
        "ds_train = create_tensorflow_dataset(features_train)\\\n",
        "            .shuffle(len(features_train), seed=seed)\\\n",
        "            .batch(batch_size, drop_remainder=False)\\\n",
        "            .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "ds_test = create_tensorflow_dataset(features_test)\\\n",
        "    .batch(batch_size, drop_remainder=False)\\\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "steps_per_epoch = math.ceil(len(features_train)/batch_size)\n",
        "validation_steps = math.ceil(len(features_test)/batch_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "converting samples:   5%|▍         | 10/220 [00:00<00:02, 99.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total Samples Processed: 220\n",
            "labels skipped: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "converting samples: 100%|██████████| 220/220 [00:01<00:00, 151.37it/s]\n",
            "generating features: 100%|██████████| 220/220 [00:11<00:00, 19.30it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{0: 'O', 1: 'B-COMPANIES WORKED AT', 2: 'I-COMPANIES WORKED AT', 3: 'B-COLLEGE NAME', 4: 'I-COLLEGE NAME', 5: 'B-DEGREE', 6: 'I-DEGREE', 7: 'B-DESIGNATION', 8: 'I-DESIGNATION', 9: 'B-EMAIL ADDRESS', 10: 'I-EMAIL ADDRESS', 11: 'B-LOCATION', 12: 'I-LOCATION', 13: 'B-NAME', 14: 'I-NAME', 15: 'B-YEARS OF EXPERIENCE', 16: 'I-YEARS OF EXPERIENCE'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KTrHhmbmH4V"
      },
      "source": [
        "@tf.function\n",
        "def train_step(x, y, model, loss_fn, optimizer):\n",
        "  #forward propagation\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(x, training=True)\n",
        "    loss_val = loss_fn(y, logits)\n",
        "  #backpropagation\n",
        "  gradients = tape.gradient(loss_val, model.trainable_variables)\n",
        "  #based on backprop, update model weights\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return loss_val\n",
        "\n",
        "@tf.function\n",
        "def evaluation_step(x, y, model, loss_fn):\n",
        "  logits= model(x, training=False)\n",
        "  loss_val = loss_fn(y, logits)\n",
        "  return loss_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTkrd2TotdO3"
      },
      "source": [
        "def align_predictions(predictions: np.ndarray, label_ids: np.ndarray, attention_masks: np.array, class_map:Dict) -> Tuple[List[int], List[int]]:\n",
        "    preds = np.argmax(predictions, axis=2)\n",
        "    batch_size, seq_len = preds.shape\n",
        "    out_label_list = [[] for _ in range(batch_size)]\n",
        "    preds_list = [[] for _ in range(batch_size)]\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        for j in range(seq_len):\n",
        "            if attention_masks[i, j] ==1:\n",
        "                out_label_list[i].append(class_map[label_ids[i][j]])\n",
        "                preds_list[i].append(class_map[preds[i][j]])\n",
        "\n",
        "    return preds_list, out_label_list\n",
        "\n",
        "def evaluate_model(model, dataset, batch_size, steps, class_map:Dict, return_report=True):\n",
        "    preds = []\n",
        "    labels = []\n",
        "    attention_masks = []\n",
        "    for i in tqdm(range(steps)):\n",
        "        data_batch, labels_batch = next(iter(dataset))\n",
        "        preds_batch = model.predict(data_batch, batch_size = batch_size, verbose=0)\n",
        "        preds.append(preds_batch)\n",
        "        labels.append(labels_batch.numpy())\n",
        "        attention_masks.append( data_batch['attention_mask'].numpy())\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    labels = np.concatenate(labels, axis=0)\n",
        "    attention_masks = np.concatenate(attention_masks, axis=0)\n",
        "    preds_list, out_label_list = align_predictions(preds, labels, attention_masks, class_map)\n",
        "    if return_report:\n",
        "        return {\n",
        "                \"precision\": precision_score(out_label_list, preds_list),\n",
        "                \"recall\": recall_score(out_label_list, preds_list),\n",
        "                \"f1\": f1_score(out_label_list, preds_list),\n",
        "                'classification_report': classification_report(out_label_list, preds_list)\n",
        "            }\n",
        "    return f1_score(out_label_list, preds_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhXWFiGyhwvQ",
        "outputId": "cef5fd2e-c9dc-48b3-cef4-ae960dc50a6a"
      },
      "source": [
        "model = build_model(len(class_map), True, dropout_rate = 0.45)\n",
        "# print(model.summary())\n",
        "optimizer = optimizers.Adam(learning_rate=1e-4)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
        "epoch_loss_metric = tf.keras.metrics.Mean()\n",
        "training_loss_history, val_loss_history = [], []\n",
        "print('steps per epochs:', steps_per_epoch )\n",
        "for epoch in range(10):\n",
        "  tf.print('Epoch ',epoch+1,' started')\n",
        "  for x_batch_train, y_batch_train in tqdm(ds_train, total= steps_per_epoch, desc='training'):\n",
        "    batch_loss = train_step(x_batch_train, y_batch_train, model, loss_fn, optimizer)\n",
        "    # if tf.math.is_nan(batch_loss):\n",
        "    #   a, b = x_batch_train, y_batch_train\n",
        "    #   raise Exception('here')\n",
        "    epoch_loss_metric.update_state(batch_loss)\n",
        "    epoch_loss_train = epoch_loss_metric.result().numpy().item()\n",
        "    epoch_loss_metric.reset_states()\n",
        "  for x_batch_val, y_batch_val in tqdm(ds_test, total= validation_steps, desc='validating'):\n",
        "    batch_loss = evaluation_step(x_batch_val, y_batch_val, model, loss_fn)\n",
        "    epoch_loss_metric.update_state(batch_loss)\n",
        "    epoch_loss_val = epoch_loss_metric.result().numpy().item()\n",
        "  result_train = evaluate_model(model, ds_train, batch_size, steps= steps_per_epoch, class_map = class_map, return_report= False)\n",
        "  result_val = evaluate_model(model, ds_test, batch_size, steps= validation_steps, class_map = class_map, return_report= False)\n",
        "  print('training loss:', epoch_loss_train, ' val loss:', epoch_loss_val)\n",
        "  print('Epoch ',epoch+1, ' Training F1:', result_train, ' validation f1:', result_val)\n",
        "  training_loss_history.append(epoch_loss_train)\n",
        "  val_loss_history.append(epoch_loss_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing TFElectraModel: ['discriminator_predictions']\n",
            "- This IS expected if you are initializing TFElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFElectraModel were initialized from the model checkpoint at google/electra-base-discriminator.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "steps per epochs: 74\n",
            "Epoch  1  started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training: 100%|██████████| 74/74 [00:58<00:00,  1.26it/s]\n",
            "validating: 100%|██████████| 19/19 [00:06<00:00,  3.03it/s]\n",
            "100%|██████████| 74/74 [00:27<00:00,  2.64it/s]\n",
            "100%|██████████| 19/19 [00:04<00:00,  4.12it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.2728392779827118  val loss: 0.14711977541446686\n",
            "Epoch  1  Training F1: 0.11469344608879492  validation f1: 0.27027027027027023\n",
            "Epoch  2  started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training: 100%|██████████| 74/74 [00:48<00:00,  1.52it/s]\n",
            "validating: 100%|██████████| 19/19 [00:03<00:00,  4.99it/s]\n",
            "100%|██████████| 74/74 [00:26<00:00,  2.78it/s]\n",
            "100%|██████████| 19/19 [00:04<00:00,  4.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.11617723107337952  val loss: 0.10369794815778732\n",
            "Epoch  2  Training F1: 0.4544  validation f1: 0.5714285714285714\n",
            "Epoch  3  started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training: 100%|██████████| 74/74 [00:48<00:00,  1.52it/s]\n",
            "validating: 100%|██████████| 19/19 [00:03<00:00,  4.97it/s]\n",
            "100%|██████████| 74/74 [00:26<00:00,  2.77it/s]\n",
            "100%|██████████| 19/19 [00:04<00:00,  4.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.11705130338668823  val loss: 0.08094348013401031\n",
            "Epoch  3  Training F1: 0.5328960852633736  validation f1: 0.6956521739130435\n",
            "Epoch  4  started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training: 100%|██████████| 74/74 [00:48<00:00,  1.52it/s]\n",
            "validating: 100%|██████████| 19/19 [00:03<00:00,  5.00it/s]\n",
            "100%|██████████| 74/74 [00:26<00:00,  2.77it/s]\n",
            "100%|██████████| 19/19 [00:04<00:00,  4.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.022395947948098183  val loss: 0.08070254325866699\n",
            "Epoch  4  Training F1: 0.7229571984435798  validation f1: 0.7843137254901961\n",
            "Epoch  5  started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training: 100%|██████████| 74/74 [00:48<00:00,  1.52it/s]\n",
            "validating: 100%|██████████| 19/19 [00:03<00:00,  4.99it/s]\n",
            "100%|██████████| 74/74 [00:26<00:00,  2.78it/s]\n",
            "100%|██████████| 19/19 [00:04<00:00,  4.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.10959555953741074  val loss: 0.08559748530387878\n",
            "Epoch  5  Training F1: 0.7819581958195819  validation f1: 0.8163265306122449\n",
            "Epoch  6  started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training: 100%|██████████| 74/74 [00:48<00:00,  1.52it/s]\n",
            "validating: 100%|██████████| 19/19 [00:03<00:00,  4.95it/s]\n",
            "100%|██████████| 74/74 [00:26<00:00,  2.78it/s]\n",
            "100%|██████████| 19/19 [00:04<00:00,  4.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.12367884069681168  val loss: 0.08858359605073929\n",
            "Epoch  6  Training F1: 0.7896857373086219  validation f1: 0.7826086956521738\n",
            "Epoch  7  started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training: 100%|██████████| 74/74 [00:48<00:00,  1.53it/s]\n",
            "validating: 100%|██████████| 19/19 [00:03<00:00,  4.99it/s]\n",
            "100%|██████████| 74/74 [00:26<00:00,  2.80it/s]\n",
            "100%|██████████| 19/19 [00:04<00:00,  4.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.10675588250160217  val loss: 0.09347495436668396\n",
            "Epoch  7  Training F1: 0.8307692307692308  validation f1: 0.75\n",
            "Epoch  8  started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training: 100%|██████████| 74/74 [00:48<00:00,  1.53it/s]\n",
            "validating: 100%|██████████| 19/19 [00:03<00:00,  4.97it/s]\n",
            "100%|██████████| 74/74 [00:26<00:00,  2.80it/s]\n",
            "100%|██████████| 19/19 [00:04<00:00,  4.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.02852211892604828  val loss: 0.11413860321044922\n",
            "Epoch  8  Training F1: 0.8050570962479608  validation f1: 0.68\n",
            "Epoch  9  started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training: 100%|██████████| 74/74 [00:48<00:00,  1.53it/s]\n",
            "validating: 100%|██████████| 19/19 [00:03<00:00,  4.97it/s]\n",
            "100%|██████████| 74/74 [00:26<00:00,  2.76it/s]\n",
            "100%|██████████| 19/19 [00:04<00:00,  4.08it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.06559143215417862  val loss: 0.13148558139801025\n",
            "Epoch  9  Training F1: 0.8801054018445322  validation f1: 0.6545454545454545\n",
            "Epoch  10  started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "training: 100%|██████████| 74/74 [00:48<00:00,  1.53it/s]\n",
            "validating: 100%|██████████| 19/19 [00:03<00:00,  4.96it/s]\n",
            "100%|██████████| 74/74 [00:26<00:00,  2.76it/s]\n",
            "100%|██████████| 19/19 [00:04<00:00,  4.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.008254670538008213  val loss: 0.12229684740304947\n",
            "Epoch  10  Training F1: 0.9423548650858545  validation f1: 0.8695652173913043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbH9w43HA3Xu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3edbbe2d-ef0a-4ff8-ce8e-53ac36696c13"
      },
      "source": [
        "result_train = evaluate_model(model, ds_train, batch_size, steps= steps_per_epoch, class_map = class_map, return_report= True)\n",
        "result_test = evaluate_model(model, ds_test, batch_size, steps= steps_per_epoch, class_map = class_map, return_report= True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:26<00:00,  2.77it/s]\n",
            "100%|██████████| 74/74 [00:18<00:00,  4.08it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LbmcFz2ORna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd4ff59-517f-4bd5-bb4a-aa9a7a6b19d6"
      },
      "source": [
        "print(result_train['classification_report'])\n",
        "print(result_test['classification_report'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "             DEGREE       0.96      0.98      0.97       245\n",
            "           LOCATION       0.98      0.97      0.97       294\n",
            "COMPANIES WORKED AT       0.91      0.96      0.94       575\n",
            "       COLLEGE NAME       0.96      0.96      0.96       265\n",
            "        DESIGNATION       0.89      0.95      0.92       370\n",
            "      EMAIL ADDRESS       0.90      0.96      0.93       187\n",
            "YEARS OF EXPERIENCE       0.94      0.92      0.93        37\n",
            "               NAME       0.99      1.00      1.00       169\n",
            "\n",
            "          micro avg       0.93      0.97      0.95      2142\n",
            "          macro avg       0.93      0.97      0.95      2142\n",
            "\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "        DESIGNATION       1.00      1.00      1.00       222\n",
            "           LOCATION       1.00      1.00      1.00       592\n",
            "             DEGREE       0.00      0.00      0.00        74\n",
            "               NAME       1.00      1.00      1.00       222\n",
            "COMPANIES WORKED AT       0.25      0.50      0.33       148\n",
            "      EMAIL ADDRESS       0.75      1.00      0.86       222\n",
            "       COLLEGE NAME       1.00      1.00      1.00       148\n",
            "\n",
            "          micro avg       0.83      0.91      0.87      1628\n",
            "          macro avg       0.85      0.91      0.87      1628\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}